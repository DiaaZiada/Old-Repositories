{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Machine_Translation_with_Attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "m3eQaBZNsF3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation with Attention"
      ]
    },
    {
      "metadata": {
        "id": "-Zs2YUeIrPU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34bf21bc-4ac8-43c2-cf61-190df0f3bb53"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AB3-cmcr42FG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download and prepare the dataset"
      ]
    },
    {
      "metadata": {
        "id": "bkQpOoHX4yTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxCDQPGM5MZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FkOA6JG25zld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-xvAt1X6FYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17tAPOzt6aps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Spanish sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "    \n",
        "    # English sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sb9ckifU6-4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = None\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = target_tensor, input_tensor ,targ_lang, inp_lang, max_length_targ, max_length_inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKkmCBCf7IeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dec88302-c950-4d23-b373-621fe93da23e"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95171, 95171, 23793, 23793)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "i9hMvqy18GVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create a tf.data dataset"
      ]
    },
    {
      "metadata": {
        "id": "n16QvpAL7-fK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DhH3R5L8YPu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Write the encoder and decoder model"
      ]
    },
    {
      "metadata": {
        "id": "XEvy-Jpr8VUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inaqrFwX9uZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RfuELu399Ps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBRnDVyXnFkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbVpUxBXnLlb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define the optimizer and the loss function"
      ]
    },
    {
      "metadata": {
        "id": "IQcKcQn1nIcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84b3GqzKniL1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checkpoints (Object-based saving)"
      ]
    },
    {
      "metadata": {
        "id": "8Bn8E8tZnTrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints/fra_eng'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whbj4YjSnwDP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "nA7SXZ82nveW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3290
        },
        "outputId": "601127d4-635f-48be-d3d1-5606b13efd16"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.6590\n",
            "Epoch 1 Batch 100 Loss 0.9313\n",
            "Epoch 1 Batch 200 Loss 0.7689\n",
            "Epoch 1 Batch 300 Loss 0.7038\n",
            "Epoch 1 Batch 400 Loss 0.6560\n",
            "Epoch 1 Batch 500 Loss 0.6097\n",
            "Epoch 1 Batch 600 Loss 0.6567\n",
            "Epoch 1 Batch 700 Loss 0.7020\n",
            "Epoch 1 Batch 800 Loss 0.6298\n",
            "Epoch 1 Batch 900 Loss 0.6445\n",
            "Epoch 1 Batch 1000 Loss 0.5997\n",
            "Epoch 1 Batch 1100 Loss 0.5947\n",
            "Epoch 1 Batch 1200 Loss 0.5942\n",
            "Epoch 1 Batch 1300 Loss 0.5831\n",
            "Epoch 1 Batch 1400 Loss 0.5554\n",
            "Epoch 1 Loss 0.6813\n",
            "Time taken for 1 epoch 2356.14852142334 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5310\n",
            "Epoch 2 Batch 100 Loss 0.5897\n",
            "Epoch 2 Batch 200 Loss 0.5992\n",
            "Epoch 2 Batch 300 Loss 0.5163\n",
            "Epoch 2 Batch 400 Loss 0.5432\n",
            "Epoch 2 Batch 500 Loss 0.5353\n",
            "Epoch 2 Batch 600 Loss 0.5015\n",
            "Epoch 2 Batch 700 Loss 0.5154\n",
            "Epoch 2 Batch 800 Loss 0.4100\n",
            "Epoch 2 Batch 900 Loss 0.4995\n",
            "Epoch 2 Batch 1000 Loss 0.4144\n",
            "Epoch 2 Batch 1100 Loss 0.3894\n",
            "Epoch 2 Batch 1200 Loss 0.4578\n",
            "Epoch 2 Batch 1300 Loss 0.4362\n",
            "Epoch 2 Batch 1400 Loss 0.3602\n",
            "Epoch 2 Loss 0.4733\n",
            "Time taken for 1 epoch 2362.8233494758606 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.3535\n",
            "Epoch 3 Batch 100 Loss 0.4280\n",
            "Epoch 3 Batch 200 Loss 0.4046\n",
            "Epoch 3 Batch 300 Loss 0.3250\n",
            "Epoch 3 Batch 400 Loss 0.2998\n",
            "Epoch 3 Batch 500 Loss 0.3441\n",
            "Epoch 3 Batch 600 Loss 0.3629\n",
            "Epoch 3 Batch 700 Loss 0.3281\n",
            "Epoch 3 Batch 800 Loss 0.3066\n",
            "Epoch 3 Batch 900 Loss 0.2824\n",
            "Epoch 3 Batch 1000 Loss 0.2918\n",
            "Epoch 3 Batch 1100 Loss 0.2884\n",
            "Epoch 3 Batch 1200 Loss 0.2479\n",
            "Epoch 3 Batch 1300 Loss 0.2994\n",
            "Epoch 3 Batch 1400 Loss 0.3057\n",
            "Epoch 3 Loss 0.3242\n",
            "Time taken for 1 epoch 2371.84353017807 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2035\n",
            "Epoch 4 Batch 100 Loss 0.2331\n",
            "Epoch 4 Batch 200 Loss 0.2554\n",
            "Epoch 4 Batch 300 Loss 0.2440\n",
            "Epoch 4 Batch 400 Loss 0.2037\n",
            "Epoch 4 Batch 500 Loss 0.2982\n",
            "Epoch 4 Batch 600 Loss 0.2633\n",
            "Epoch 4 Batch 700 Loss 0.2836\n",
            "Epoch 4 Batch 800 Loss 0.2326\n",
            "Epoch 4 Batch 900 Loss 0.2170\n",
            "Epoch 4 Batch 1000 Loss 0.2279\n",
            "Epoch 4 Batch 1100 Loss 0.2338\n",
            "Epoch 4 Batch 1200 Loss 0.2302\n",
            "Epoch 4 Batch 1300 Loss 0.2380\n",
            "Epoch 4 Batch 1400 Loss 0.1878\n",
            "Epoch 4 Loss 0.2362\n",
            "Time taken for 1 epoch 2375.2424297332764 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1628\n",
            "Epoch 5 Batch 100 Loss 0.1895\n",
            "Epoch 5 Batch 200 Loss 0.1976\n",
            "Epoch 5 Batch 300 Loss 0.1664\n",
            "Epoch 5 Batch 400 Loss 0.1545\n",
            "Epoch 5 Batch 500 Loss 0.1611\n",
            "Epoch 5 Batch 600 Loss 0.1810\n",
            "Epoch 5 Batch 700 Loss 0.1620\n",
            "Epoch 5 Batch 800 Loss 0.2269\n",
            "Epoch 5 Batch 900 Loss 0.1788\n",
            "Epoch 5 Batch 1000 Loss 0.1556\n",
            "Epoch 5 Batch 1100 Loss 0.1860\n",
            "Epoch 5 Batch 1200 Loss 0.1650\n",
            "Epoch 5 Batch 1300 Loss 0.1763\n",
            "Epoch 5 Batch 1400 Loss 0.1474\n",
            "Epoch 5 Loss 0.1822\n",
            "Time taken for 1 epoch 2379.3615152835846 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1496\n",
            "Epoch 6 Batch 100 Loss 0.0956\n",
            "Epoch 6 Batch 200 Loss 0.1868\n",
            "Epoch 6 Batch 300 Loss 0.1477\n",
            "Epoch 6 Batch 400 Loss 0.1167\n",
            "Epoch 6 Batch 500 Loss 0.1328\n",
            "Epoch 6 Batch 600 Loss 0.1285\n",
            "Epoch 6 Batch 700 Loss 0.1616\n",
            "Epoch 6 Batch 800 Loss 0.2037\n",
            "Epoch 6 Batch 900 Loss 0.1210\n",
            "Epoch 6 Batch 1000 Loss 0.1306\n",
            "Epoch 6 Batch 1100 Loss 0.1497\n",
            "Epoch 6 Batch 1200 Loss 0.1121\n",
            "Epoch 6 Batch 1300 Loss 0.1417\n",
            "Epoch 6 Batch 1400 Loss 0.1514\n",
            "Epoch 6 Loss 0.1447\n",
            "Time taken for 1 epoch 2381.5290310382843 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1431\n",
            "Epoch 7 Batch 100 Loss 0.1244\n",
            "Epoch 7 Batch 200 Loss 0.1257\n",
            "Epoch 7 Batch 300 Loss 0.1237\n",
            "Epoch 7 Batch 400 Loss 0.1163\n",
            "Epoch 7 Batch 500 Loss 0.1328\n",
            "Epoch 7 Batch 600 Loss 0.1260\n",
            "Epoch 7 Batch 700 Loss 0.1514\n",
            "Epoch 7 Batch 800 Loss 0.1329\n",
            "Epoch 7 Batch 900 Loss 0.1591\n",
            "Epoch 7 Batch 1000 Loss 0.1169\n",
            "Epoch 7 Batch 1100 Loss 0.1084\n",
            "Epoch 7 Batch 1200 Loss 0.1218\n",
            "Epoch 7 Batch 1300 Loss 0.1255\n",
            "Epoch 7 Batch 1400 Loss 0.1178\n",
            "Epoch 7 Loss 0.1191\n",
            "Time taken for 1 epoch 2378.1313982009888 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0760\n",
            "Epoch 8 Batch 100 Loss 0.1006\n",
            "Epoch 8 Batch 200 Loss 0.0996\n",
            "Epoch 8 Batch 300 Loss 0.1045\n",
            "Epoch 8 Batch 400 Loss 0.1343\n",
            "Epoch 8 Batch 500 Loss 0.0957\n",
            "Epoch 8 Batch 600 Loss 0.0910\n",
            "Epoch 8 Batch 700 Loss 0.0707\n",
            "Epoch 8 Batch 800 Loss 0.1423\n",
            "Epoch 8 Batch 900 Loss 0.0725\n",
            "Epoch 8 Batch 1000 Loss 0.1609\n",
            "Epoch 8 Batch 1100 Loss 0.1086\n",
            "Epoch 8 Batch 1200 Loss 0.0943\n",
            "Epoch 8 Batch 1300 Loss 0.1046\n",
            "Epoch 8 Batch 1400 Loss 0.1219\n",
            "Epoch 8 Loss 0.1001\n",
            "Time taken for 1 epoch 2379.1967828273773 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0829\n",
            "Epoch 9 Batch 100 Loss 0.0859\n",
            "Epoch 9 Batch 200 Loss 0.1029\n",
            "Epoch 9 Batch 300 Loss 0.1385\n",
            "Epoch 9 Batch 400 Loss 0.0923\n",
            "Epoch 9 Batch 500 Loss 0.0982\n",
            "Epoch 9 Batch 600 Loss 0.0619\n",
            "Epoch 9 Batch 700 Loss 0.0778\n",
            "Epoch 9 Batch 800 Loss 0.0779\n",
            "Epoch 9 Batch 900 Loss 0.0764\n",
            "Epoch 9 Batch 1000 Loss 0.0866\n",
            "Epoch 9 Batch 1100 Loss 0.1255\n",
            "Epoch 9 Batch 1200 Loss 0.0944\n",
            "Epoch 9 Batch 1300 Loss 0.0870\n",
            "Epoch 9 Batch 1400 Loss 0.1120\n",
            "Epoch 9 Loss 0.0846\n",
            "Time taken for 1 epoch 2379.035696744919 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0684\n",
            "Epoch 10 Batch 100 Loss 0.0764\n",
            "Epoch 10 Batch 200 Loss 0.0552\n",
            "Epoch 10 Batch 300 Loss 0.0854\n",
            "Epoch 10 Batch 400 Loss 0.0710\n",
            "Epoch 10 Batch 500 Loss 0.0661\n",
            "Epoch 10 Batch 600 Loss 0.0829\n",
            "Epoch 10 Batch 700 Loss 0.0624\n",
            "Epoch 10 Batch 800 Loss 0.0861\n",
            "Epoch 10 Batch 900 Loss 0.0663\n",
            "Epoch 10 Batch 1000 Loss 0.0727\n",
            "Epoch 10 Batch 1100 Loss 0.0640\n",
            "Epoch 10 Batch 1200 Loss 0.0680\n",
            "Epoch 10 Batch 1300 Loss 0.0700\n",
            "Epoch 10 Batch 1400 Loss 0.0732\n",
            "Epoch 10 Loss 0.0731\n",
            "Time taken for 1 epoch 2380.806496620178 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dvx32Z8apw5N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Translate"
      ]
    },
    {
      "metadata": {
        "id": "Il6fGcUboOFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word2idx.get(i) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weigths to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6w9iN_2vp_IV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AbHRzjSQqGbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6ZoTGrnqPh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "metadata": {
        "id": "3V_mgd20qMP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfb79e98-6360-4be0-c2de-783a22ccd886"
      },
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f9561fa9828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "dy9VoPTJqS5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f4a4fdc4-87cb-48a6-eb56-983146202b66"
      },
      "cell_type": "code",
      "source": [
        "translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s too cold out here . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0KF6-BfqVLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2e570dd6-f448-4147-ee6c-831419491c59"
      },
      "cell_type": "code",
      "source": [
        "translate('esta es mi vida.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DJQ0PgaTqhFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "69a2da92-87e9-45bd-883e-314d342f68e9"
      },
      "cell_type": "code",
      "source": [
        "translate('多todavia estan en casa?', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> 多 todavia estan en casa ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gy5SaPPumMgp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}