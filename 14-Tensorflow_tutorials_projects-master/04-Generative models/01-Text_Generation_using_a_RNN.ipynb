{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation_using_a_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cM-qJdhVHUJB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install unidecode library\n",
        "A helpful library to convert unicode to ASCII."
      ]
    },
    {
      "metadata": {
        "id": "1F2a08hD_Dwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "55432028-03d7-477e-dda9-8634e372caf6"
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0lws6HLHaj-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import tensorflow and enable eager execution\n"
      ]
    },
    {
      "metadata": {
        "id": "s29sO397HTqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "# Note: Once you enable eager execution, it cannot be disabled. \n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import unidecode\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5E4YLr1HipX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the dataset"
      ]
    },
    {
      "metadata": {
        "id": "tWCFnYzlHgVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a4bddf33-647c-4705-f4fe-aeaf01fc8628"
      },
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y-5k11r9How2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ]
    },
    {
      "metadata": {
        "id": "DtTuJUKqHle1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24125829-7fd6-4345-8e78-714b14ef7a5a"
      },
      "cell_type": "code",
      "source": [
        "text = unidecode.unidecode(open(path_to_file).read())\n",
        "# length of text is the number of characters in it\n",
        "print (len(text))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "McfwUBIjHvPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unique contains all the unique characters in the file\n",
        "unique = sorted(set(text))\n",
        "# creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(unique)}\n",
        "idx2char = {i:u for i, u in enumerate(unique)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbZzwPxdH5ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting the maximum length sentence we want for a single input in characters\n",
        "max_length = 100\n",
        "\n",
        "# length of the vocabulary in chars\n",
        "vocab_size = len(unique)\n",
        "\n",
        "# the embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN (here GRU) units\n",
        "units = 1024\n",
        "\n",
        "# batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size to shuffle our dataset\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKeXkAlrIRSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating the input and output tensors"
      ]
    },
    {
      "metadata": {
        "id": "-Mxt27moIFDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6c65ae4e-26c4-40b3-de9b-2382883db7c3"
      },
      "cell_type": "code",
      "source": [
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "for f in range(0, len(text)-max_length, max_length):\n",
        "    inps = text[f:f+max_length]\n",
        "    targ = text[f+1:f+1+max_length]\n",
        "\n",
        "    input_text.append([char2idx[i] for i in inps])\n",
        "    target_text.append([char2idx[t] for t in targ])\n",
        "    \n",
        "print (np.array(input_text).shape)\n",
        "print (np.array(target_text).shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11153, 100)\n",
            "(11153, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WvJmbIDFJH-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating batches and shuffling them using tf.data"
      ]
    },
    {
      "metadata": {
        "id": "15jlY8gJJFQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_QgZYlEJTrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating the model"
      ]
    },
    {
      "metadata": {
        "id": "5-38mNvoJNsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "    self.batch_sz = batch_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          return_state=True, \n",
        "                                          recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # output shape == (batch_size, max_length, hidden_size) \n",
        "    # states shape == (batch_size, hidden_size)\n",
        "\n",
        "    # states variable to preserve the state of the model\n",
        "    # this will be used to pass at every step to the model while training\n",
        "    output, states = self.gru(x, initial_state=hidden)\n",
        "\n",
        "\n",
        "    # reshaping the output so that we can pass it to the Dense layer\n",
        "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # The dense layer will output predictions for every time_steps(max_length)\n",
        "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_g1NLo2fLkxs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Call the model and set the optimizer and the loss function"
      ]
    },
    {
      "metadata": {
        "id": "HTpn4j1uLi6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LEml_70ULxtY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RGhRTL_L_3-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checkpoints (Object-based saving)"
      ]
    },
    {
      "metadata": {
        "id": "dyRZjnXZL3Q5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaheVk9cOBbe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ]
    },
    {
      "metadata": {
        "id": "DNeW4QdhMgMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1697
        },
        "outputId": "1f188143-8e56-4178-befe-49657a9ff446"
      },
      "cell_type": "code",
      "source": [
        "# Training step\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions, hidden = model(inp, hidden)\n",
        "              \n",
        "              # reshaping the target because that's how the \n",
        "              # loss function expects it\n",
        "              target = tf.reshape(target, (-1,))\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1755\n",
            "Epoch 1 Batch 100 Loss 2.3879\n",
            "Epoch 1 Loss 2.1674\n",
            "Time taken for 1 epoch 25.53898024559021 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1254\n",
            "Epoch 2 Batch 100 Loss 1.8889\n",
            "Epoch 2 Loss 1.7644\n",
            "Time taken for 1 epoch 24.338879346847534 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7419\n",
            "Epoch 3 Batch 100 Loss 1.6441\n",
            "Epoch 3 Loss 1.5556\n",
            "Time taken for 1 epoch 24.51798939704895 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5959\n",
            "Epoch 4 Batch 100 Loss 1.4926\n",
            "Epoch 4 Loss 1.4534\n",
            "Time taken for 1 epoch 24.543392419815063 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4584\n",
            "Epoch 5 Batch 100 Loss 1.4318\n",
            "Epoch 5 Loss 1.3883\n",
            "Time taken for 1 epoch 24.696858167648315 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3502\n",
            "Epoch 6 Batch 100 Loss 1.4288\n",
            "Epoch 6 Loss 1.3787\n",
            "Time taken for 1 epoch 24.573727130889893 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3336\n",
            "Epoch 7 Batch 100 Loss 1.2970\n",
            "Epoch 7 Loss 1.3222\n",
            "Time taken for 1 epoch 24.53632664680481 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2960\n",
            "Epoch 8 Batch 100 Loss 1.3224\n",
            "Epoch 8 Loss 1.3231\n",
            "Time taken for 1 epoch 24.69237971305847 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2399\n",
            "Epoch 9 Batch 100 Loss 1.2725\n",
            "Epoch 9 Loss 1.2731\n",
            "Time taken for 1 epoch 25.196274042129517 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2206\n",
            "Epoch 10 Batch 100 Loss 1.2343\n",
            "Epoch 10 Loss 1.2272\n",
            "Time taken for 1 epoch 24.95027804374695 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.1854\n",
            "Epoch 11 Batch 100 Loss 1.2448\n",
            "Epoch 11 Loss 1.2004\n",
            "Time taken for 1 epoch 24.933556079864502 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.0990\n",
            "Epoch 12 Batch 100 Loss 1.1751\n",
            "Epoch 12 Loss 1.2018\n",
            "Time taken for 1 epoch 24.872349500656128 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.0756\n",
            "Epoch 13 Batch 100 Loss 1.1366\n",
            "Epoch 13 Loss 1.1395\n",
            "Time taken for 1 epoch 24.747027158737183 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.0828\n",
            "Epoch 14 Batch 100 Loss 1.1234\n",
            "Epoch 14 Loss 1.1420\n",
            "Time taken for 1 epoch 24.549745798110962 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.0087\n",
            "Epoch 15 Batch 100 Loss 1.1183\n",
            "Epoch 15 Loss 1.0878\n",
            "Time taken for 1 epoch 24.801796197891235 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.9938\n",
            "Epoch 16 Batch 100 Loss 1.0453\n",
            "Epoch 16 Loss 1.0852\n",
            "Time taken for 1 epoch 24.699490785598755 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.9532\n",
            "Epoch 17 Batch 100 Loss 1.0278\n",
            "Epoch 17 Loss 1.0632\n",
            "Time taken for 1 epoch 25.07541847229004 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.9333\n",
            "Epoch 18 Batch 100 Loss 1.0087\n",
            "Epoch 18 Loss 1.0124\n",
            "Time taken for 1 epoch 25.09708595275879 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.8756\n",
            "Epoch 19 Batch 100 Loss 0.9507\n",
            "Epoch 19 Loss 0.9707\n",
            "Time taken for 1 epoch 24.96747612953186 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.8355\n",
            "Epoch 20 Batch 100 Loss 0.9056\n",
            "Epoch 20 Loss 0.9524\n",
            "Time taken for 1 epoch 24.98086404800415 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2TWw0j9DSA-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Restore the latest checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "PM7xoiKgP7Oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4b9500e0-5aab-4cb3-e575-32afa2510b4d"
      },
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f6755dcec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "mk-lRKGbSK2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting using our trained model"
      ]
    },
    {
      "metadata": {
        "id": "z9hVWUsxSHda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "9284bc5e-b7f0-426a-a2a3-93ec09905fe7"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step(generating text using the model learned)\n",
        "\n",
        "# number of characters to generate\n",
        "num_generate = 1000\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'Q'\n",
        "# converting our start string to numbers(vectorizing!) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# empty string to store our results\n",
        "text_generated = ''\n",
        "\n",
        "# low temperatures results in more predictable text.\n",
        "# higher temperatures results in more surprising text\n",
        "# experiment to find the best setting\n",
        "temperature = 1.0\n",
        "\n",
        "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
        "hidden = [tf.zeros((1, units))]\n",
        "for i in range(num_generate):\n",
        "    predictions, hidden = model(input_eval, hidden)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated += idx2char[predicted_id]\n",
        "\n",
        "print (start_string + text_generated)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUKET:\n",
            "That hath pass'd it me ediction: he is\n",
            "the county alone;\n",
            "Then once again, all men are firmulture, allame, betwixt them!\n",
            "\n",
            "RUTLAND:\n",
            "Did not being thy heart willing in Somersation!\n",
            "Officiours! Mine honest wealth signs?\n",
            "Urth Caius Marcius.\n",
            "\n",
            "BRUTUS:\n",
            "Come away; or on not all as those that regree went to silence, and the boldness is rin Claudio?\n",
            "Will you not arm me, Kate, the kingly look to fly,\n",
            "To fight on England's quarrellows' countrymen or sour record\n",
            "To omplain unto the root of spring of them.\n",
            "\n",
            "TRANIO:\n",
            "Mistress it pleasem, why leaving so far off\n",
            "And Julietbe myself to hear\n",
            "To eye our flesh is outwilling heaven for them\n",
            "Shall do appear in doubled call you at the gates of York,\n",
            "And art thou mad with trie oclain and nawling jow\n",
            "And lies thy hands the wisds of all he is come to have some:\n",
            "And in me, and what say you to love?\n",
            "\n",
            "VAll it please you\n",
            "Liest thou not quickly for her son: you know hee\n",
            "Of what you see, or slemple,\n",
            "Her moon, the means than the law whom you shall be Goubt,\n",
            "When wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hk7vhC4rUWIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}